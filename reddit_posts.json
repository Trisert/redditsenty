[
  {
    "id": "1qpewj7",
    "title": "AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model",
    "selftext": "Hi\u00a0[r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)\n\nToday we are having\u00a0**Kimi**, the research lab behind the\u00a0**Kimi**\u00a0**K2.5**. We\u2019re excited to have them open up and answer your questions directly.\n\nOur participants today:\n\n* [u/ComfortableAsk4494](https://www.reddit.com/user/ComfortableAsk4494/)\n* [u/zxytim](https://www.reddit.com/user/zxytim/)\n* [u/ppwwyyxx](https://www.reddit.com/user/ppwwyyxx/)\n\n**The AMA will run from 8 AM \u2013 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.**\n\nhttps://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;format=png&amp;auto=webp&amp;s=98c89b5d86ee1197799532fead6a84da2223b389\n\n&gt;  \nThanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.",
    "author": "nekofneko",
    "score": 269,
    "ups": 269,
    "downs": 0,
    "num_comments": 241,
    "created_utc": "2026-01-28 16:46:40",
    "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/",
    "subreddit": "LocalLLaMA"
  },
  {
    "id": "1mpk2va",
    "title": "Announcing LocalLlama discord server &amp; bot!",
    "selftext": "INVITE: https://discord.gg/rC922KfEwj\n\nThere used to be one old discord server for the subreddit but it was deleted by the previous mod.\n\nWhy?\nThe subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).\n\nWe have a discord bot to test out open source models.\n\nBetter contest and events organization.\n\nBest for quick questions or showcasing your rig!",
    "author": "HOLUPREDICTIONS",
    "score": 114,
    "ups": 114,
    "downs": 0,
    "num_comments": 65,
    "created_utc": "2025-08-14 01:21:05",
    "permalink": "https://reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/",
    "url": "https://www.reddit.com/gallery/1mpk2va",
    "subreddit": "LocalLLaMA"
  },
  {
    "id": "1qtwqq2",
    "title": "Unreal",
    "selftext": "",
    "author": "analgerianabroad",
    "score": 1598,
    "ups": 1598,
    "downs": 0,
    "num_comments": 61,
    "created_utc": "2026-02-02 15:37:43",
    "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtwqq2/unreal/",
    "url": "https://i.redd.it/6a90dq5re3hg1.png",
    "subreddit": "LocalLLaMA"
  },
  {
    "id": "1qtvp74",
    "title": "GLM-5 Coming in February! It's confirmed.",
    "selftext": "Twitter Link: [https://x.com/jietang/status/2018246490775498791?s=20](https://x.com/jietang/status/2018246490775498791?s=20)",
    "author": "Difficult-Cap-7527",
    "score": 368,
    "ups": 368,
    "downs": 0,
    "num_comments": 86,
    "created_utc": "2026-02-02 14:56:14",
    "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/",
    "url": "https://i.redd.it/rq0meza173hg1.jpeg",
    "subreddit": "LocalLLaMA"
  },
  {
    "id": "1qtvo4r",
    "title": "128GB devices have a new local LLM king: Step-3.5-Flash-int4",
    "selftext": "Here's the HF Repo: http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4 (this is a GGUF repo)\n\nI've been running this LLM for about an hour and it has handled all coding tests I've thrown at it in chat mode. IMO this is as good if not better than GLM 4.7, Minimax 2.1 while being much more efficient. Later I will try some agentic coding to see how it performs, but I already have high hopes for it.\n\nI use a 128GB M1 ultra mac studio and can run it at full context (256k). Not only it is fast, but also super efficient in RAM usage.\n\n*Update: I ran llama-bench with up to 100k prefill. Here are the results:\n\n    % llama-bench -m step3p5_flash_Q4_K_S.gguf -fa 1 -t 1 -ngl 99 -b 2048 -ub 2048 -d 0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000\n    ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.024 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.024 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_device_init: has unified memory    = true\n    ggml_metal_device_init: has bfloat            = true\n    ggml_metal_device_init: has tensor            = false\n    ggml_metal_device_init: use residency sets    = true\n    ggml_metal_device_init: use shared buffers    = true\n    ggml_metal_device_init: recommendedMaxWorkingSetSize  = 134217.73 MB\n    | model                          |       size |     params | backend    | threads | n_ubatch | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | --------------: | -------------------: |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |           pp512 |        281.09 \u00b1 1.57 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |           tg128 |         34.70 \u00b1 0.01 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d10000 |        248.10 \u00b1 1.08 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d10000 |         31.69 \u00b1 0.04 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d20000 |        222.18 \u00b1 0.49 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d20000 |         30.02 \u00b1 0.04 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d30000 |        200.68 \u00b1 0.78 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d30000 |         28.62 \u00b1 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d40000 |        182.86 \u00b1 0.55 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d40000 |         26.89 \u00b1 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d50000 |        167.61 \u00b1 0.23 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d50000 |         25.37 \u00b1 0.03 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d60000 |        154.50 \u00b1 0.19 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d60000 |         24.10 \u00b1 0.01 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d70000 |        143.60 \u00b1 0.29 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d70000 |         22.95 \u00b1 0.01 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d80000 |        134.02 \u00b1 0.35 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d80000 |         21.87 \u00b1 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d90000 |        125.34 \u00b1 0.19 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d90000 |         20.66 \u00b1 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 | pp512 @ d100000 |        117.72 \u00b1 0.07 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 | tg128 @ d100000 |         19.78 \u00b1 0.01 |\n    \n    build: a0dce6f (24)\n\nThis is still very usable with 100k prefill, so a good option for CLI coding agents!\n\nYou need to build a llama.cpp fork to run it, instructions at the HF repo. Though this model is so good that I believe it will soon be supported by llama.cpp upstream.",
    "author": "tarruda",
    "score": 139,
    "ups": 139,
    "downs": 0,
    "num_comments": 57,
    "created_utc": "2026-02-02 14:55:00",
    "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/",
    "subreddit": "LocalLLaMA"
  },
  {
    "id": "1qtu8x1",
    "title": "GLM 5 Coming Soon",
    "selftext": "https://preview.redd.it/3i8wkkp8w2hg1.png?width=635&amp;format=png&amp;auto=webp&amp;s=bd400f6ceedc90114cc90feedd2126e2bad951dc\n\n[https://x.com/jietang/status/2018246490775498791](https://x.com/jietang/status/2018246490775498791)",
    "author": "External_Mood4719",
    "score": 112,
    "ups": 112,
    "downs": 0,
    "num_comments": 29,
    "created_utc": "2026-02-02 13:54:04",
    "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtu8x1/glm_5_coming_soon/",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qtu8x1/glm_5_coming_soon/",
    "subreddit": "LocalLLaMA"
  },
  {
    "id": "1qttq5w",
    "title": "devstral small is faster and better than glm 4.7 flash for local agentic coding.",
    "selftext": "i just realised token per second is not the only thing that matters in agentic coding. glm 4.7 flash is almlst 3x faster but it keeps thinking for way more than 3 times the total tokens it generates so yes at the end devstral small finishes the task slighter faster than glm 4.7 flash. while obiously being much much better at agentic coding.\n\ntoken efficiency of devstral small has to be discussed more often. its incredble.",
    "author": "theghost3172",
    "score": 75,
    "ups": 75,
    "downs": 0,
    "num_comments": 32,
    "created_utc": "2026-02-02 13:28:47",
    "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/",
    "subreddit": "LocalLLaMA"
  },
  {
    "id": "1qtqspu",
    "title": "1 Day Left Until ACE-Step 1.5 \u2014 Open-Source Music Gen That Runs on &lt;4GB VRAM  Open suno alternative (and yes, i made this frontend)",
    "selftext": "An open-source model with quality approaching Suno v4.5/v5... running locally on a potato GPU. No subscriptions. No API limits. Just you and your creativity.  \n  \nWe're so lucky to be in this era of open-source AI. A year ago this was unthinkable.",
    "author": "ExcellentTrust4433",
    "score": 107,
    "ups": 107,
    "downs": 0,
    "num_comments": 36,
    "created_utc": "2026-02-02 10:47:32",
    "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtqspu/1_day_left_until_acestep_15_opensource_music_gen/",
    "url": "https://v.redd.it/2geqqfooy1hg1",
    "subreddit": "LocalLLaMA"
  },
  {
    "id": "1qtjhc8",
    "title": "Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2",
    "selftext": "The newly released Stepfun model Step-3.5-Flash outperforms DeepSeek v3.2 on multiple coding and agentic benchmarks, despite using far fewer parameters.\n\nStep-3.5-Flash: 196B total / 11B active parameters\n\nDeepSeek v3.2: 671B total / 37B active parameters\n\nHugging Face: https://huggingface.co/stepfun-ai/Step-3.5-Flash",
    "author": "ResearchCrafty1804",
    "score": 320,
    "ups": 320,
    "downs": 0,
    "num_comments": 139,
    "created_utc": "2026-02-02 04:07:42",
    "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/",
    "url": "https://www.reddit.com/gallery/1qtjhc8",
    "subreddit": "LocalLLaMA"
  },
  {
    "id": "1qu1j8f",
    "title": "ggml-cpu: FA split across kv for faster TG",
    "selftext": "CPU Flash-Attention decoding speed-up (long contexts).",
    "author": "jacek2023",
    "score": 21,
    "ups": 21,
    "downs": 0,
    "num_comments": 18,
    "created_utc": "2026-02-02 18:30:24",
    "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qu1j8f/ggmlcpu_fa_split_across_kv_for_faster_tg/",
    "url": "https://github.com/ggml-org/llama.cpp/pull/19209",
    "subreddit": "LocalLLaMA"
  }
]